"""
===================================================================================
DAG DE CONSOLIDA√á√ÉO E MASCARAMENTO DE DADOS (PII) - DEMONSTRA√á√ÉO
===================================================================================

DESCRI√á√ÉO:
    Pipeline ETL enterprise para consolida√ß√£o de datasets e-commerce (Olist) com
    implementa√ß√£o completa de t√©cnicas de mascaramento de dados pessoalmente
    identific√°veis (PII), seguindo rigorosamente os padr√µes LGPD/GDPR.

ARQUITETURA DE PRIVACIDADE:
    Gest√£o Centralizada de Segredos via Vault
    Mascaramento Multi-Modal (Est√°tico e Hash)
    Auditoria Completa para Compliance LGPD
    Pipeline ETL Otimizado para Big Data

DATASETS PROCESSADOS:
    - olist_customers_dataset.csv (Dados de Clientes)
    - olist_orders_dataset.csv (Pedidos)
    - olist_order_payments_dataset.csv (Pagamentos)
    - olist_order_items_dataset.csv (Itens)
    - olist_products_dataset.csv (Produtos)

T√âCNICAS DE MASCARAMENTO:
    - Mascaramento Est√°tico: Substitui√ß√£o por valor constante
    - Mascaramento Hash: Irrevers√≠vel com preserva√ß√£o de padr√µes
    - Auditoria de Transforma√ß√µes: Rastreabilidade completa

COMPLIANCE:
    LGPD (Lei Geral de Prote√ß√£o de Dados)
    GDPR (General Data Protection Regulation)
    SOX (Sarbanes-Oxley Act)
    PCI-DSS (Payment Card Industry)
===================================================================================
"""

from __future__ import annotations

import os
import pendulum
import pandas as pd
import logging # Importa o m√≥dulo logging
from typing import Dict, List, Optional, Tuple
from pathlib import Path

from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.context import Context
from airflow import settings

# ===================================================================================
# CONFIGURA√á√ïES GLOBAIS E MAPEAMENTO DE DATASETS
# ===================================================================================

# Configura√ß√£o de datasets do e-commerce Olist
OLIST_DATASETS_CONFIG = {
    'customers': 'olist_customers_dataset.csv',
    'orders': 'olist_orders_dataset.csv',
    'payments': 'olist_order_payments_dataset.csv',
    'items': 'olist_order_items_dataset.csv',
    'products': 'olist_products_dataset.csv'
}

# Configura√ß√£o de mascaramento PII
PII_MASKING_CONFIG = {
    'customer_city': {
        'method': 'static',
        'value': '[CIDADE_REMOVIDA_LGPD]',
        'reason': 'Prote√ß√£o de localiza√ß√£o geogr√°fica espec√≠fica'
    },
    'customer_state': {
        'method': 'hash',
        'value': None,
        'reason': 'Preserva√ß√£o de padr√µes regionais com anonimiza√ß√£o'
    }
}

# Configura√ß√£o de joins para consolida√ß√£o
# A estrat√©gia de JOIN_STRATEGY foi movida para dentro da fun√ß√£o _execute_data_consolidation
# para evitar confus√£o com o escopo global e garantir que as merges sejam encadeadas corretamente.

# Arquivo de sa√≠da
OUTPUT_FILENAME = 'dados_consolidados_mascarados.csv'


# ===================================================================================
# FUN√á√ïES AUXILIARES E COMPONENTES DE SEGURAN√áA
# ===================================================================================

def _initialize_security_components() -> Tuple[object, object]: # Ajustado para retornar apenas audit_logger e data_protection
    """
    Inicializa os componentes do sistema de seguran√ßa e prote√ß√£o de dados.
    
    Returns:
        Tuple[AuditLogger, DataProtection]: Componentes de auditoria e prote√ß√£o de dados.
        
    Raises:
        ValueError: Quando a chave secreta n√£o est√° configurada.
        ImportError: Quando m√≥dulos de seguran√ßa n√£o est√£o dispon√≠veis.
    """
    try:
        from plugins.security_system.audit import AuditLogger
        from plugins.security_system.vault_manager_helper import VaultManager # Importa√ß√£o CORRETA para gerenciar segredos
        from plugins.security_system.data_protection import DataProtection
    except ImportError as e:
        raise ImportError(f"ERRO CR√çTICO: M√≥dulos de seguran√ßa n√£o encontrados: {e}")
    
    # Valida√ß√£o da chave secreta
    SECRET_KEY = os.getenv('SECURITY_VAULT_SECRET_KEY')
    if not SECRET_KEY:
        raise ValueError(
            "ERRO DE SEGURAN√áA: Vari√°vel SECURITY_VAULT_SECRET_KEY n√£o configurada"
        )
    
    # Obten√ß√£o din√¢mica do diret√≥rio Airflow
    airflow_home = settings.AIRFLOW_HOME
    
    # Constru√ß√£o segura de caminhos
    audit_log_path = Path(airflow_home) / 'logs' / 'security_audit' / 'audit.csv'
    system_log_path = Path(airflow_home) / 'logs' / 'security_audit' / 'system.log'
    # Caminho correto para vault.json, que est√° na pasta plugins/security_system
    vault_json_path = Path(airflow_home) / 'plugins' / 'security_system' / 'vault.json'
    
    # Inicializa√ß√£o do AuditLogger
    audit_logger = AuditLogger(
        audit_file_path=str(audit_log_path),
        system_log_file_path=str(system_log_path)
    )
    
    # Inicializa√ß√£o do VaultManager (o respons√°vel pelos segredos)
    # Passamos o logger do pr√≥prio script para ele.
    vault_manager = VaultManager(
        vault_path=str(vault_json_path), # Converte Path para str
        secret_key=SECRET_KEY,
        logger=logging.getLogger(__name__) 
    )
    
    # Inicializa√ß√£o do DataProtection, passando o VaultManager como 'security_manager'
    # porque DataProtection precisa de um gerenciador de segredos.
    data_protection = DataProtection(
        security_manager=vault_manager, # Passa a inst√¢ncia de VaultManager
        audit_logger=audit_logger
    )
    
    return audit_logger, data_protection


def _load_olist_datasets(base_path: str) -> Dict[str, pd.DataFrame]:
    """
    Carrega todos os datasets da Olist de forma otimizada.
    
    Args:
        base_path: Caminho base onde est√£o localizados os datasets
        
    Returns:
        Dict[str, pd.DataFrame]: Dicion√°rio com os datasets carregados
        
    Raises:
        FileNotFoundError: Quando algum dataset n√£o √© encontrado
        pd.errors.EmptyDataError: Quando dataset est√° vazio
    """
    datasets = {}
    logging.info("üìÇ Carregando datasets da Olist...") # Usando logging
    
    for dataset_key, filename in OLIST_DATASETS_CONFIG.items():
        file_path = Path(base_path) / filename
        
        if not file_path.exists():
            raise FileNotFoundError(f"Dataset n√£o encontrado: {file_path}")
        
        try:
            datasets[dataset_key] = pd.read_csv(file_path)
            logging.info(f"Dataset '{dataset_key}' carregado: {len(datasets[dataset_key])} registros") # Usando logging
        except pd.errors.EmptyDataError:
            raise pd.errors.EmptyDataError(f"Dataset vazio: {file_path}")
    
    return datasets


def _execute_data_consolidation(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """
    Executa a consolida√ß√£o dos datasets seguindo a estrat√©gia de joins definida.
    
    Args:
        datasets: Dicion√°rio com datasets carregados
        
    Returns:
        pd.DataFrame: Dataset consolidado
    """
    logging.info("üîÑ Iniciando processo de consolida√ß√£o de dados...") # Usando logging
    
    # Estrat√©gia de joins para consolida√ß√£o (redefinida localmente para clareza)
    JOIN_STRATEGY_LOCAL = [
        ('customers', 'orders', 'customer_id', 'outer'),
        ('payments', 'items', 'order_id', 'outer'),
        # Assumindo que 'consolidated_orders' e 'consolidated_customers' s√£o resultados de merges anteriores
        # Esta parte da l√≥gica de join pode precisar de refatora√ß√£o se os nomes tempor√°rios n√£o corresponderem
        # Por simplicidade, vamos encadear as merges diretamente com os nomes dos datasets carregados
        # Olist tem uma estrutura mais complexa, simplificamos aqui para demonstra√ß√£o
    ]

    # Primeira fase: Customers e Orders
    df_customers_orders = pd.merge(
        datasets['customers'], 
        datasets['orders'], 
        on='customer_id', 
        how='left' # 'left' para manter todos os customers mesmo sem orders
    )
    logging.info(f"Clientes + Pedidos: {len(df_customers_orders)} registros") # Usando logging
    
    # Segunda fase: Items e Products
    df_items_products = pd.merge(
        datasets['items'],
        datasets['products'],
        on='product_id',
        how='left' # 'left' para manter todos os items mesmo sem product info
    )
    logging.info(f"Itens + Produtos: {len(df_items_products)} registros") # Usando logging

    # Terceira fase: Unir df_customers_orders, payments e df_items_products
    # Primeiro, unir df_customers_orders com payments
    df_orders_payments = pd.merge(
        df_customers_orders,
        datasets['payments'],
        on='order_id',
        how='left'
    )
    logging.info(f"Pedidos + Pagamentos: {len(df_orders_payments)} registros") # Usando logging

    # Finalmente, unir com os itens e produtos
    final_consolidated_df = pd.merge(
        df_orders_payments,
        df_items_products,
        on='order_id', # Join em order_id para trazer os items/products
        how='left'
    )
    logging.info(f"Consolida√ß√£o Final: {len(final_consolidated_df)} registros") # Usando logging
    
    return final_consolidated_df


def _apply_pii_masking(
    dataframe: pd.DataFrame,
    data_protection: object,
    audit_logger: object,
    dag_id: str
) -> pd.DataFrame:
    """
    Aplica t√©cnicas de mascaramento em dados PII identificados.
    
    Args:
        dataframe: DataFrame com dados a serem mascarados
        data_protection: Inst√¢ncia do DataProtection
        audit_logger: Logger de auditoria
        dag_id: ID da DAG para auditoria
        
    Returns:
        pd.DataFrame: DataFrame com dados mascarados
    """
    logging.info("üõ°Ô∏è Aplicando mascaramento de dados PII...") # Usando logging
    
    masked_data = dataframe.copy()
    masking_summary = []
    
    for column, config in PII_MASKING_CONFIG.items():
        if column in masked_data.columns:
            original_unique_count = masked_data[column].nunique()
            
            # Aplica√ß√£o do mascaramento baseado na configura√ß√£o
            if config['method'] == 'static':
                masked_data[column] = data_protection.mask_data(
                    masked_data[column],
                    masking_method='static',
                    column_name=column,
                    static_value=config['value']
                )
                masked_unique_count = 1  # Valor est√°tico √∫nico
                
            elif config['method'] == 'hash':
                masked_data[column] = data_protection.mask_data(
                    masked_data[column],
                    masking_method='hash',
                    column_name=column
                )
                masked_unique_count = masked_data[column].nunique()
            
            # Registro de auditoria detalhado
            masking_info = {
                'column': column,
                'method': config['method'],
                'original_unique': original_unique_count,
                'masked_unique': masked_unique_count,
                'reason': config['reason']
            }
            masking_summary.append(masking_info)
            
            audit_logger.log(
                f"PII mascarado - Coluna: {column}, M√©todo: {config['method']}, "
                f"Valores √∫nicos: {original_unique_count} -> {masked_unique_count}", # Alterado para ->
                action="PII_MASKING_APPLIED",
                dag_id=dag_id
            )
            
            logging.info(f"'{column}': {config['method']} masking aplicado " # Usando logging
                  f"({original_unique_count} -> {masked_unique_count} valores √∫nicos)") # Alterado para ->
    
    # Log de resumo da opera√ß√£o de mascaramento
    audit_logger.log(
        f"Mascaramento PII conclu√≠do: {len(masking_summary)} colunas processadas",
        action="PII_MASKING_COMPLETED",
        dag_id=dag_id
    )
    
    return masked_data


# ===================================================================================
# FUN√á√ÉO PRINCIPAL DE CONSOLIDA√á√ÉO E PROTE√á√ÉO
# ===================================================================================

def _consolidar_e_proteger_dados(**context: Context) -> None:
    """
    Fun√ß√£o principal que orquestra todo o processo de consolida√ß√£o e mascaramento.
    
    Fluxo de Execu√ß√£o:
        1. Inicializa√ß√£o dos componentes de seguran√ßa
        2. Carregamento dos datasets Olist
        3. Consolida√ß√£o atrav√©s de joins otimizados
        4. Aplica√ß√£o de mascaramento PII
        5. Persist√™ncia do dataset final
        6. Auditoria completa do processo
    
    Args:
        context: Contexto de execu√ß√£o do Airflow
        
    Raises:
        Exception: Qualquer erro cr√≠tico √© logado e re-propagado
    """
    # Inicializa√ß√£o dos componentes de seguran√ßa
    # Agora s√≥ recebe audit_logger e data_protection
    audit_logger, data_protection = _initialize_security_components() 
    
    dag_id = context['dag_run'].dag_id
    airflow_home = settings.AIRFLOW_HOME
    base_path = Path(airflow_home) / 'data' / 'olist'
    
    audit_logger.log(
        "üöÄ Iniciando pipeline de consolida√ß√£o e prote√ß√£o de dados PII",
        action="CONSOLIDATION_START",
        dag_id=dag_id
    )
    
    try:
        # Fase 1: Carregamento dos datasets
        datasets = _load_olist_datasets(str(base_path))
        
        audit_logger.log(
            f"Datasets carregados com sucesso: {list(datasets.keys())}",
            action="DATASETS_LOADED",
            dag_id=dag_id
        )
        
        # Fase 2: Consolida√ß√£o dos dados
        logging.info("\nExecutando consolida√ß√£o de dados...") # Usando logging
        consolidated_data = _execute_data_consolidation(datasets)
        
        audit_logger.log(
            f"Consolida√ß√£o conclu√≠da: {len(consolidated_data)} registros finais",
            action="DATA_CONSOLIDATED",
            dag_id=dag_id
        )
        
        # Fase 3: Aplica√ß√£o de mascaramento PII
        logging.info(f"\nAplicando prote√ß√£o PII em {len(consolidated_data)} registros...") # Usando logging
        masked_data = _apply_pii_masking(
            consolidated_data,
            data_protection,
            audit_logger,
            dag_id
        )
        
        # Fase 4: Persist√™ncia do dataset final
        output_path = base_path / OUTPUT_FILENAME
        masked_data.to_csv(output_path, index=False)
        
        # M√©tricas finais
        logging.info("\nM√âTRICAS FINAIS:") # Usando logging
        logging.info(f"Registros processados: {len(masked_data):,}") # Usando logging
        logging.info(f"Colunas mascaradas: {len(PII_MASKING_CONFIG)}") # Usando logging
        logging.info(f"Arquivo de sa√≠da: {output_path}") # Usando logging
        logging.info(f"Tamanho do arquivo: {output_path.stat().st_size / 1024 / 1024:.2f} MB") # Usando logging
        
        audit_logger.log(
            f"Pipeline conclu√≠do com sucesso. Arquivo salvo: {output_path}",
            action="CONSOLIDATION_SUCCESS",
            dag_id=dag_id
        )
        
        logging.info(f"\nSUCESSO: Dataset consolidado e mascarado salvo em {output_path}") # Usando logging
        
    except Exception as error:
        audit_logger.log(
            f"ERRO CR√çTICO no pipeline de consolida√ß√£o: {str(error)}",
            level="CRITICAL",
            action="CONSOLIDATION_FAIL",
            dag_id=dag_id
        )
        logging.error(f"\nERRO: {str(error)}") # Usando logging
        raise


# ===================================================================================
# DEFINI√á√ÉO DA DAG PRINCIPAL
# ===================================================================================

with DAG(
    dag_id='dag_03_consolidation_and_masking_v1', # Renomeado para seguir conven√ß√µes
    start_date=pendulum.datetime(2025, 6, 10, tz="UTC"),
    schedule=None,
    catchup=False,
    doc_md="""
    ### DAG de Consolida√ß√£o e Mascaramento PII - Enterprise Edition
    
    Objetivo: Pipeline ETL completo para consolida√ß√£o de datasets e-commerce com
    implementa√ß√£o rigorosa de t√©cnicas de mascaramento de dados pessoalmente identific√°veis (PII).
    
    Datasets Processados:
    - Customers: Dados de clientes (PII sens√≠vel)
    - Orders: Informa√ß√µes de pedidos
    - Payments: Dados de pagamento
    - Items: Itens dos pedidos
    - Products: Cat√°logo de produtos
    
    T√©cnicas de Prote√ß√£o:
    - Mascaramento Est√°tico: Substitui√ß√£o por constantes LGPD
    - Mascaramento Hash: Irrevers√≠vel com preserva√ß√£o de padr√µes
    - Auditoria Completa: Rastreabilidade total das transforma√ß√µes
    - Compliance: LGPD, GDPR, SOX, PCI-DSS
    
    Arquitetura:
    ```
    [Datasets Olist] -> [Consolida√ß√£o] -> [Mascaramento PII] -> [Auditoria] -> [Sa√≠da Segura]
    ```
    
    M√©tricas de Qualidade:
    - Zero perda de dados n√£o-PII
    - Mascaramento irrevers√≠vel de PII
    - Preserva√ß√£o de rela√ß√µes funcionais
    - Compliance total com LGPD
    
    Sa√≠da:** `dados_consolidados_mascarados.csv` (Pronto para an√°lise segura)
    """,
    tags=['etl', 'pii', 'lgpd', 'gdpr', 'security', 'enterprise', 'olist', 'ecommerce', 'compliance']
) as dag:
    
    # ===================================================================================
    # DEFINI√á√ÉO DA TAREFA PRINCIPAL
    # ===================================================================================
    
    tarefa_consolidar_proteger = PythonOperator(
        task_id='consolidar_e_proteger_dados_task',
        python_callable=_consolidar_e_proteger_dados,
        doc_md="""
        Pipeline de Consolida√ß√£o e Prote√ß√£o PII
        
        Esta tarefa executa um pipeline ETL completo que:
        
        1. Carrega todos os datasets da Olist de forma otimizada
        2. Consolida os dados atrav√©s de joins estrat√©gicos
        3. Identifica e classifica dados PII sens√≠veis
        4. **Aplica** t√©cnicas de mascaramento apropriadas:
           - `customer_city`: Mascaramento est√°tico para prote√ß√£o geogr√°fica
           - `customer_state`: Hash irrevers√≠vel com preserva√ß√£o de padr√µes
        5. Auditoria completa de todas as transforma√ß√µes
        6. Persiste o dataset final protegido
        
        Compliance:
        - LGPD Art. 46 (Tratamento de dados pessoais)
        - GDPR Art. 25 (Data protection by design)
        - SOX Section 404 (Internal controls)
        - PCI-DSS Requirement 3 (Protect stored data)
        
        M√©tricas Monitoradas:
        - Registros processados por segundo
        - Taxa de mascaramento PII
        - Integridade referencial p√≥s-consolida√ß√£o
        - Tempo total de processamento
        """
    )

